{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "836d397b-a90d-4ec9-b38e-327880a703b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import yfinance as yf\n",
    "\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl import config_tickers\n",
    "from finrl.config import INDICATORS\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6e765083-2248-4a6c-a514-2fccc0584c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = config_tickers.DOW_30_TICKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "92a99a4f-bb5f-4e24-8062-a59bca517d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_START_DATE = '2009-01-01'\n",
    "TRAIN_END_DATE = '2022-12-31'\n",
    "TRADE_START_DATE = '2023-01-01'\n",
    "TRADE_END_DATE = '2024-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b47d6b0d-abf5-40bd-b487-12d892272ac4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (110651, 8)\n"
     ]
    }
   ],
   "source": [
    "df_raw = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "                     end_date = TRADE_END_DATE,\n",
    "                     ticker_list = config_tickers.DOW_30_TICKER).fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c2129c34-4a94-4be5-8232-dca4b3e91a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (3773, 8)\n",
      "Successfully added vix\n",
      "Successfully added turbulence index\n"
     ]
    }
   ],
   "source": [
    "fe = FeatureEngineer(use_technical_indicator=True,\n",
    "                     tech_indicator_list = INDICATORS,\n",
    "                     use_vix=True,\n",
    "                     use_turbulence=True,\n",
    "                     user_defined_feature = False)\n",
    "\n",
    "processed = fe.preprocess_data(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f7529652-9b42-4ac2-a40e-23f59c3f791a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ticker = processed[\"tic\"].unique().tolist()\n",
    "list_date = list(pd.date_range(processed['date'].min(),processed['date'].max()).astype(str))\n",
    "combination = list(itertools.product(list_date,list_ticker))\n",
    "\n",
    "processed_full = pd.DataFrame(combination,columns=[\"date\",\"tic\"]).merge(processed,on=[\"date\",\"tic\"],how=\"left\")\n",
    "processed_full = processed_full[processed_full['date'].isin(processed['date'])]\n",
    "processed_full = processed_full.sort_values(['date','tic'])\n",
    "\n",
    "processed_full = processed_full.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "51e94b43-7f2d-4a80-ae0b-10cc9e04585e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102196\n",
      "7221\n"
     ]
    }
   ],
   "source": [
    "train = data_split(processed_full, TRAIN_START_DATE,TRAIN_END_DATE)\n",
    "trade = data_split(processed_full, TRADE_START_DATE,TRADE_END_DATE)\n",
    "print(len(train))\n",
    "print(len(trade))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "06dd00ba-bc51-472d-8016-d6b0cd0444a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_data.csv')\n",
    "trade.to_csv('trade_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ad259d3b-06e5-421e-bd98-68aae4187bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "\n",
    "check_and_make_directories([TRAINED_MODEL_DIR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fa9b54a2-c8d7-4621-9057-7ec8988286ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "\n",
    "# If you are not using the data generated from part 1 of this tutorial, make sure \n",
    "# it has the columns and index in the form that could be make into the environment. \n",
    "# Then you can comment and skip the following two lines.\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = ['']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2512ce94-4161-4765-9eaa-5ad8f441d3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 29, State Space: 291\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a5e7f23b-30f3-45b4-858b-be0d4b249d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4\n",
    "}\n",
    "\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2f93764e-8762-4464-bc2c-1ee1c4736afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "07a684ba-f8e2-4721-a604-63ed3bc7d43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "\n",
    "# Set the corresponding values to 'True' for the algorithms that you want to use\n",
    "if_using_a2c = True\n",
    "if_using_ddpg = True\n",
    "if_using_ppo = True\n",
    "if_using_td3 = True\n",
    "if_using_sac = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dad38e06-7cd4-4dab-807a-7b24612fc33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_a2c = agent.get_model(\"a2c\")\n",
    "\n",
    "if if_using_a2c:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/a2c'\n",
    "  new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_a2c.set_logger(new_logger_a2c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "21b9911f-51b0-48c6-b580-76aff48d7867",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 185          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 2            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -41.1        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -41.5        |\n",
      "|    reward             | 0.0018689639 |\n",
      "|    std                | 0.998        |\n",
      "|    value_loss         | 1.66         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 187        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | -0.776     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -45.5      |\n",
      "|    reward             | 0.24864058 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.21       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 187      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.1    |\n",
      "|    explained_variance | 0.0114   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -203     |\n",
      "|    reward             | 4.381096 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 25.8     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 187        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0.108      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | 198        |\n",
      "|    reward             | -0.5615505 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 26.5       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 186         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 13          |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | 0.0119      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | 577         |\n",
      "|    reward             | -11.9583435 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 210         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 186       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | -0.00464  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 304       |\n",
      "|    reward             | 6.0958285 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 80.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 186        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0.0927     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 434        |\n",
      "|    reward             | -11.492373 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 127        |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 186          |\n",
      "|    iterations         | 800          |\n",
      "|    time_elapsed       | 21           |\n",
      "|    total_timesteps    | 4000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -41.1        |\n",
      "|    explained_variance | -2.5e-06     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 799          |\n",
      "|    policy_loss        | -76.8        |\n",
      "|    reward             | -0.033557013 |\n",
      "|    std                | 1            |\n",
      "|    value_loss         | 3.65         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 186       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0.00121   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -316      |\n",
      "|    reward             | 1.6507586 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 70.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 186        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0.0303     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 67.3       |\n",
      "|    reward             | -0.4518119 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 8.62       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 185      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 29       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.1    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 11.7     |\n",
      "|    reward             | 8.452753 |\n",
      "|    std                | 0.998    |\n",
      "|    value_loss         | 1.83     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 183       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 32        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | -0.00264  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -432      |\n",
      "|    reward             | 2.8235672 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 198       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 35         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -36        |\n",
      "|    reward             | -22.933805 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 19.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 181       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 38        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 866       |\n",
      "|    reward             | 4.7269764 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 475       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 180        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 41         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 30         |\n",
      "|    reward             | -2.6137488 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 1          |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 178         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 44          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 118         |\n",
      "|    reward             | -0.74635595 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 10.5        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 178       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 47        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -265      |\n",
      "|    reward             | -8.417386 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 50.6      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 177         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 50          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | -117        |\n",
      "|    reward             | -0.04720219 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 15.3        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 177       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 53        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 196       |\n",
      "|    reward             | 2.7624137 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 36.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 177        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 56         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -218       |\n",
      "|    reward             | -1.5866166 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 61.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 176        |\n",
      "|    iterations         | 2100       |\n",
      "|    time_elapsed       | 59         |\n",
      "|    total_timesteps    | 10500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2099       |\n",
      "|    policy_loss        | -1.36e+03  |\n",
      "|    reward             | -3.7200913 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 1.31e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 176       |\n",
      "|    iterations         | 2200      |\n",
      "|    time_elapsed       | 62        |\n",
      "|    total_timesteps    | 11000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.1     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2199      |\n",
      "|    policy_loss        | 58.1      |\n",
      "|    reward             | 0.5928611 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 7.42      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 176        |\n",
      "|    iterations         | 2300       |\n",
      "|    time_elapsed       | 65         |\n",
      "|    total_timesteps    | 11500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2299       |\n",
      "|    policy_loss        | -78.6      |\n",
      "|    reward             | -0.7832789 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.97       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 175       |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 68        |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2399      |\n",
      "|    policy_loss        | 173       |\n",
      "|    reward             | 1.9369575 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 25.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 175        |\n",
      "|    iterations         | 2500       |\n",
      "|    time_elapsed       | 71         |\n",
      "|    total_timesteps    | 12500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2499       |\n",
      "|    policy_loss        | 25.7       |\n",
      "|    reward             | -1.8963436 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.37       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 175       |\n",
      "|    iterations         | 2600      |\n",
      "|    time_elapsed       | 73        |\n",
      "|    total_timesteps    | 13000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0.000997  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2599      |\n",
      "|    policy_loss        | 152       |\n",
      "|    reward             | 2.3404384 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 16.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 176       |\n",
      "|    iterations         | 2700      |\n",
      "|    time_elapsed       | 76        |\n",
      "|    total_timesteps    | 13500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2699      |\n",
      "|    policy_loss        | 498       |\n",
      "|    reward             | 1.0648327 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 176       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 176       |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 79        |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | -20.5     |\n",
      "|    reward             | 5.3825126 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 34.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 177        |\n",
      "|    iterations         | 2900       |\n",
      "|    time_elapsed       | 81         |\n",
      "|    total_timesteps    | 14500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2899       |\n",
      "|    policy_loss        | 137        |\n",
      "|    reward             | -4.1696963 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 13.8       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 177      |\n",
      "|    iterations         | 3000     |\n",
      "|    time_elapsed       | 84       |\n",
      "|    total_timesteps    | 15000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | 115      |\n",
      "|    reward             | 3.203877 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 11.7     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 177        |\n",
      "|    iterations         | 3100       |\n",
      "|    time_elapsed       | 87         |\n",
      "|    total_timesteps    | 15500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3099       |\n",
      "|    policy_loss        | -23.2      |\n",
      "|    reward             | -1.2678759 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.45       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 178        |\n",
      "|    iterations         | 3200       |\n",
      "|    time_elapsed       | 89         |\n",
      "|    total_timesteps    | 16000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3199       |\n",
      "|    policy_loss        | -68.9      |\n",
      "|    reward             | -0.5456916 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.87       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 178        |\n",
      "|    iterations         | 3300       |\n",
      "|    time_elapsed       | 92         |\n",
      "|    total_timesteps    | 16500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.2      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3299       |\n",
      "|    policy_loss        | 167        |\n",
      "|    reward             | -0.5815996 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 21.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 178        |\n",
      "|    iterations         | 3400       |\n",
      "|    time_elapsed       | 95         |\n",
      "|    total_timesteps    | 17000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3399       |\n",
      "|    policy_loss        | 512        |\n",
      "|    reward             | -2.2605731 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 152        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 178        |\n",
      "|    iterations         | 3500       |\n",
      "|    time_elapsed       | 97         |\n",
      "|    total_timesteps    | 17500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3499       |\n",
      "|    policy_loss        | 302        |\n",
      "|    reward             | -3.8340685 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 82.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 179       |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 100       |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3599      |\n",
      "|    policy_loss        | -252      |\n",
      "|    reward             | 1.3959274 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 44.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 179       |\n",
      "|    iterations         | 3700      |\n",
      "|    time_elapsed       | 103       |\n",
      "|    total_timesteps    | 18500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3699      |\n",
      "|    policy_loss        | -78.7     |\n",
      "|    reward             | 2.0874925 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 6.52      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 179       |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 105       |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.4     |\n",
      "|    explained_variance | 0.0004    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | 62.1      |\n",
      "|    reward             | 1.4122835 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 5.44      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 179        |\n",
      "|    iterations         | 3900       |\n",
      "|    time_elapsed       | 108        |\n",
      "|    total_timesteps    | 19500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3899       |\n",
      "|    policy_loss        | -146       |\n",
      "|    reward             | -1.2692219 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 20.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 179        |\n",
      "|    iterations         | 4000       |\n",
      "|    time_elapsed       | 111        |\n",
      "|    total_timesteps    | 20000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3999       |\n",
      "|    policy_loss        | 204        |\n",
      "|    reward             | -2.0218723 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 35.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 180       |\n",
      "|    iterations         | 4100      |\n",
      "|    time_elapsed       | 113       |\n",
      "|    total_timesteps    | 20500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4099      |\n",
      "|    policy_loss        | 1.19e+03  |\n",
      "|    reward             | 10.323341 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 806       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 180       |\n",
      "|    iterations         | 4200      |\n",
      "|    time_elapsed       | 116       |\n",
      "|    total_timesteps    | 21000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4199      |\n",
      "|    policy_loss        | 1.38e+03  |\n",
      "|    reward             | 18.582731 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.69e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 180        |\n",
      "|    iterations         | 4300       |\n",
      "|    time_elapsed       | 119        |\n",
      "|    total_timesteps    | 21500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4299       |\n",
      "|    policy_loss        | -78.8      |\n",
      "|    reward             | 0.31378865 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 7.52       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 180        |\n",
      "|    iterations         | 4400       |\n",
      "|    time_elapsed       | 121        |\n",
      "|    total_timesteps    | 22000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4399       |\n",
      "|    policy_loss        | -273       |\n",
      "|    reward             | -0.8648694 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 51.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 180       |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 124       |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4499      |\n",
      "|    policy_loss        | -65.2     |\n",
      "|    reward             | 1.8106928 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 5.68      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 180       |\n",
      "|    iterations         | 4600      |\n",
      "|    time_elapsed       | 127       |\n",
      "|    total_timesteps    | 23000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4599      |\n",
      "|    policy_loss        | -137      |\n",
      "|    reward             | 1.2566075 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 11.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 180       |\n",
      "|    iterations         | 4700      |\n",
      "|    time_elapsed       | 130       |\n",
      "|    total_timesteps    | 23500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4699      |\n",
      "|    policy_loss        | 960       |\n",
      "|    reward             | 1.3182344 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 536       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 180       |\n",
      "|    iterations         | 4800      |\n",
      "|    time_elapsed       | 132       |\n",
      "|    total_timesteps    | 24000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4799      |\n",
      "|    policy_loss        | 200       |\n",
      "|    reward             | 21.548265 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 232       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 180       |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 135       |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | -3.38e+03 |\n",
      "|    reward             | 11.588454 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 8.67e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 180       |\n",
      "|    iterations         | 5000      |\n",
      "|    time_elapsed       | 138       |\n",
      "|    total_timesteps    | 25000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4999      |\n",
      "|    policy_loss        | -23.4     |\n",
      "|    reward             | 2.8942008 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 6.07      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 181       |\n",
      "|    iterations         | 5100      |\n",
      "|    time_elapsed       | 140       |\n",
      "|    total_timesteps    | 25500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | -0.000172 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5099      |\n",
      "|    policy_loss        | -239      |\n",
      "|    reward             | -0.824689 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 35.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 181        |\n",
      "|    iterations         | 5200       |\n",
      "|    time_elapsed       | 143        |\n",
      "|    total_timesteps    | 26000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.6      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5199       |\n",
      "|    policy_loss        | -190       |\n",
      "|    reward             | 0.71998227 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 24.6       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 181         |\n",
      "|    iterations         | 5300        |\n",
      "|    time_elapsed       | 146         |\n",
      "|    total_timesteps    | 26500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5299        |\n",
      "|    policy_loss        | -172        |\n",
      "|    reward             | -0.19169979 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 18.1        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 181      |\n",
      "|    iterations         | 5400     |\n",
      "|    time_elapsed       | 149      |\n",
      "|    total_timesteps    | 27000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.6    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5399     |\n",
      "|    policy_loss        | 19.9     |\n",
      "|    reward             | 7.214511 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 27.7     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 181       |\n",
      "|    iterations         | 5500      |\n",
      "|    time_elapsed       | 151       |\n",
      "|    total_timesteps    | 27500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5499      |\n",
      "|    policy_loss        | 897       |\n",
      "|    reward             | -7.846336 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.73e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 181       |\n",
      "|    iterations         | 5600      |\n",
      "|    time_elapsed       | 154       |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5599      |\n",
      "|    policy_loss        | 2.29e+03  |\n",
      "|    reward             | 24.558434 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 3.97e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 181       |\n",
      "|    iterations         | 5700      |\n",
      "|    time_elapsed       | 157       |\n",
      "|    total_timesteps    | 28500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5699      |\n",
      "|    policy_loss        | 37.8      |\n",
      "|    reward             | 0.7352747 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.95      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 181       |\n",
      "|    iterations         | 5800      |\n",
      "|    time_elapsed       | 159       |\n",
      "|    total_timesteps    | 29000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5799      |\n",
      "|    policy_loss        | 29.7      |\n",
      "|    reward             | 1.3013257 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.52      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 181       |\n",
      "|    iterations         | 5900      |\n",
      "|    time_elapsed       | 162       |\n",
      "|    total_timesteps    | 29500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5899      |\n",
      "|    policy_loss        | 28.9      |\n",
      "|    reward             | 2.8010724 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 3.16      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 181      |\n",
      "|    iterations         | 6000     |\n",
      "|    time_elapsed       | 165      |\n",
      "|    total_timesteps    | 30000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.7    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5999     |\n",
      "|    policy_loss        | 137      |\n",
      "|    reward             | 0.837297 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 19.1     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 181        |\n",
      "|    iterations         | 6100       |\n",
      "|    time_elapsed       | 168        |\n",
      "|    total_timesteps    | 30500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6099       |\n",
      "|    policy_loss        | 229        |\n",
      "|    reward             | -2.3114624 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 67.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 181       |\n",
      "|    iterations         | 6200      |\n",
      "|    time_elapsed       | 170       |\n",
      "|    total_timesteps    | 31000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6199      |\n",
      "|    policy_loss        | -1.59e+03 |\n",
      "|    reward             | 41.987064 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.69e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 181        |\n",
      "|    iterations         | 6300       |\n",
      "|    time_elapsed       | 173        |\n",
      "|    total_timesteps    | 31500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6299       |\n",
      "|    policy_loss        | -109       |\n",
      "|    reward             | -20.322231 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 445        |\n",
      "--------------------------------------\n",
      "day: 3523, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 10903311.25\n",
      "total_reward: 9903311.25\n",
      "total_cost: 13222.59\n",
      "total_trades: 57652\n",
      "Sharpe: 1.018\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 181         |\n",
      "|    iterations         | 6400        |\n",
      "|    time_elapsed       | 176         |\n",
      "|    total_timesteps    | 32000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6399        |\n",
      "|    policy_loss        | 50.8        |\n",
      "|    reward             | -0.18923245 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 1.87        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 181       |\n",
      "|    iterations         | 6500      |\n",
      "|    time_elapsed       | 178       |\n",
      "|    total_timesteps    | 32500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | -0.0464   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6499      |\n",
      "|    policy_loss        | 29.5      |\n",
      "|    reward             | 1.7103494 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 0.733     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 181       |\n",
      "|    iterations         | 6600      |\n",
      "|    time_elapsed       | 181       |\n",
      "|    total_timesteps    | 33000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6599      |\n",
      "|    policy_loss        | -288      |\n",
      "|    reward             | 1.3482448 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 56.7      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 181         |\n",
      "|    iterations         | 6700        |\n",
      "|    time_elapsed       | 184         |\n",
      "|    total_timesteps    | 33500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6699        |\n",
      "|    policy_loss        | -175        |\n",
      "|    reward             | -0.28061762 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 35.5        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 181        |\n",
      "|    iterations         | 6800       |\n",
      "|    time_elapsed       | 187        |\n",
      "|    total_timesteps    | 34000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6799       |\n",
      "|    policy_loss        | 201        |\n",
      "|    reward             | -11.261264 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 49.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 181        |\n",
      "|    iterations         | 6900       |\n",
      "|    time_elapsed       | 189        |\n",
      "|    total_timesteps    | 34500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6899       |\n",
      "|    policy_loss        | 582        |\n",
      "|    reward             | -21.653452 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 208        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 181        |\n",
      "|    iterations         | 7000       |\n",
      "|    time_elapsed       | 192        |\n",
      "|    total_timesteps    | 35000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6999       |\n",
      "|    policy_loss        | -397       |\n",
      "|    reward             | -15.999413 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 888        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 181       |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 195       |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7099      |\n",
      "|    policy_loss        | 85.4      |\n",
      "|    reward             | 0.4616168 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 5.64      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 181        |\n",
      "|    iterations         | 7200       |\n",
      "|    time_elapsed       | 198        |\n",
      "|    total_timesteps    | 36000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 2.8e-06    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7199       |\n",
      "|    policy_loss        | -58.7      |\n",
      "|    reward             | 0.20987578 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 3.07       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 181        |\n",
      "|    iterations         | 7300       |\n",
      "|    time_elapsed       | 201        |\n",
      "|    total_timesteps    | 36500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7299       |\n",
      "|    policy_loss        | 39.8       |\n",
      "|    reward             | -0.6215451 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 4.52       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 181        |\n",
      "|    iterations         | 7400       |\n",
      "|    time_elapsed       | 204        |\n",
      "|    total_timesteps    | 37000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7399       |\n",
      "|    policy_loss        | -75.4      |\n",
      "|    reward             | -3.2170148 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 29.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 181        |\n",
      "|    iterations         | 7500       |\n",
      "|    time_elapsed       | 206        |\n",
      "|    total_timesteps    | 37500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7499       |\n",
      "|    policy_loss        | 203        |\n",
      "|    reward             | -2.5308735 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 22.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 181       |\n",
      "|    iterations         | 7600      |\n",
      "|    time_elapsed       | 209       |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7599      |\n",
      "|    policy_loss        | 409       |\n",
      "|    reward             | 2.6692767 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 102       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 181        |\n",
      "|    iterations         | 7700       |\n",
      "|    time_elapsed       | 212        |\n",
      "|    total_timesteps    | 38500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7699       |\n",
      "|    policy_loss        | 1.99e+03   |\n",
      "|    reward             | -12.826107 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.66e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 181       |\n",
      "|    iterations         | 7800      |\n",
      "|    time_elapsed       | 215       |\n",
      "|    total_timesteps    | 39000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7799      |\n",
      "|    policy_loss        | -54.9     |\n",
      "|    reward             | 1.5265518 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 3.13      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 181       |\n",
      "|    iterations         | 7900      |\n",
      "|    time_elapsed       | 217       |\n",
      "|    total_timesteps    | 39500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7899      |\n",
      "|    policy_loss        | -178      |\n",
      "|    reward             | 0.7848695 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 23.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 181       |\n",
      "|    iterations         | 8000      |\n",
      "|    time_elapsed       | 220       |\n",
      "|    total_timesteps    | 40000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7999      |\n",
      "|    policy_loss        | 45.2      |\n",
      "|    reward             | 0.8411491 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.8       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 181        |\n",
      "|    iterations         | 8100       |\n",
      "|    time_elapsed       | 223        |\n",
      "|    total_timesteps    | 40500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8099       |\n",
      "|    policy_loss        | -133       |\n",
      "|    reward             | 0.38395926 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 30.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 181        |\n",
      "|    iterations         | 8200       |\n",
      "|    time_elapsed       | 226        |\n",
      "|    total_timesteps    | 41000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8199       |\n",
      "|    policy_loss        | 5          |\n",
      "|    reward             | -1.8180969 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.211      |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 181          |\n",
      "|    iterations         | 8300         |\n",
      "|    time_elapsed       | 228          |\n",
      "|    total_timesteps    | 41500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -41.7        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8299         |\n",
      "|    policy_loss        | 382          |\n",
      "|    reward             | -0.022860238 |\n",
      "|    std                | 1.02         |\n",
      "|    value_loss         | 79.4         |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 181      |\n",
      "|    iterations         | 8400     |\n",
      "|    time_elapsed       | 231      |\n",
      "|    total_timesteps    | 42000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.7    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8399     |\n",
      "|    policy_loss        | 431      |\n",
      "|    reward             | 0.819522 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 160      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 181        |\n",
      "|    iterations         | 8500       |\n",
      "|    time_elapsed       | 234        |\n",
      "|    total_timesteps    | 42500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | -0.00139   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8499       |\n",
      "|    policy_loss        | -211       |\n",
      "|    reward             | 0.14997745 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 31         |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 181         |\n",
      "|    iterations         | 8600        |\n",
      "|    time_elapsed       | 237         |\n",
      "|    total_timesteps    | 43000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.8       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8599        |\n",
      "|    policy_loss        | 55.6        |\n",
      "|    reward             | -0.29551762 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 3           |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 181       |\n",
      "|    iterations         | 8700      |\n",
      "|    time_elapsed       | 239       |\n",
      "|    total_timesteps    | 43500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8699      |\n",
      "|    policy_loss        | -53.4     |\n",
      "|    reward             | 1.5767461 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 3.06      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 181        |\n",
      "|    iterations         | 8800       |\n",
      "|    time_elapsed       | 242        |\n",
      "|    total_timesteps    | 44000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8799       |\n",
      "|    policy_loss        | -42.4      |\n",
      "|    reward             | -3.8177025 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 5.03       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 181       |\n",
      "|    iterations         | 8900      |\n",
      "|    time_elapsed       | 245       |\n",
      "|    total_timesteps    | 44500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | -29.2     |\n",
      "|    reward             | 5.5867133 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 3.4       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 181      |\n",
      "|    iterations         | 9000     |\n",
      "|    time_elapsed       | 247      |\n",
      "|    total_timesteps    | 45000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.8    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8999     |\n",
      "|    policy_loss        | 240      |\n",
      "|    reward             | 7.507052 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 71.9     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 181       |\n",
      "|    iterations         | 9100      |\n",
      "|    time_elapsed       | 250       |\n",
      "|    total_timesteps    | 45500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9099      |\n",
      "|    policy_loss        | -1.04e+03 |\n",
      "|    reward             | 7.456705  |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 756       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 181       |\n",
      "|    iterations         | 9200      |\n",
      "|    time_elapsed       | 253       |\n",
      "|    total_timesteps    | 46000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9199      |\n",
      "|    policy_loss        | -101      |\n",
      "|    reward             | -4.534342 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 7.63      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 181       |\n",
      "|    iterations         | 9300      |\n",
      "|    time_elapsed       | 255       |\n",
      "|    total_timesteps    | 46500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9299      |\n",
      "|    policy_loss        | 67.2      |\n",
      "|    reward             | 3.8504832 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 9.43      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 181        |\n",
      "|    iterations         | 9400       |\n",
      "|    time_elapsed       | 258        |\n",
      "|    total_timesteps    | 47000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9399       |\n",
      "|    policy_loss        | -58.5      |\n",
      "|    reward             | -0.3361862 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 4.33       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 9500       |\n",
      "|    time_elapsed       | 260        |\n",
      "|    total_timesteps    | 47500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9499       |\n",
      "|    policy_loss        | 111        |\n",
      "|    reward             | -2.2155771 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 13.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 9600      |\n",
      "|    time_elapsed       | 263       |\n",
      "|    total_timesteps    | 48000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9599      |\n",
      "|    policy_loss        | -166      |\n",
      "|    reward             | 1.3193645 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 21        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 9700      |\n",
      "|    time_elapsed       | 266       |\n",
      "|    total_timesteps    | 48500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9699      |\n",
      "|    policy_loss        | 433       |\n",
      "|    reward             | 4.8943048 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 141       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 9800      |\n",
      "|    time_elapsed       | 269       |\n",
      "|    total_timesteps    | 49000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9799      |\n",
      "|    policy_loss        | 356       |\n",
      "|    reward             | 3.3286872 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 201       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 181        |\n",
      "|    iterations         | 9900       |\n",
      "|    time_elapsed       | 272        |\n",
      "|    total_timesteps    | 49500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9899       |\n",
      "|    policy_loss        | 24.5       |\n",
      "|    reward             | 0.11298036 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 4.79       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 181        |\n",
      "|    iterations         | 10000      |\n",
      "|    time_elapsed       | 274        |\n",
      "|    total_timesteps    | 50000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | 0.0236     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9999       |\n",
      "|    policy_loss        | 161        |\n",
      "|    reward             | -2.8113263 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 29.9       |\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_a2c = agent.train_model(model=model_a2c, \n",
    "                             tb_log_name='a2c',\n",
    "                             total_timesteps=50000) if if_using_a2c else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "efeda507-52ab-4c7b-9606-95d607ae92e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c8695d5a-e52d-449f-b589-59400327129a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "Logging to results/ddpg\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_ddpg = agent.get_model(\"ddpg\")\n",
    "\n",
    "if if_using_ddpg:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ddpg'\n",
    "  new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ddpg.set_logger(new_logger_ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f1f1bcf9-e06b-49dc-8585-e78572107108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 3523, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7288347.61\n",
      "total_reward: 6288347.61\n",
      "total_cost: 1118.13\n",
      "total_trades: 66888\n",
      "Sharpe: 0.859\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 101        |\n",
      "|    time_elapsed    | 139        |\n",
      "|    total_timesteps | 14096      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -35.8      |\n",
      "|    critic_loss     | 22.9       |\n",
      "|    learning_rate   | 0.001      |\n",
      "|    n_updates       | 13995      |\n",
      "|    reward          | -1.7623498 |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 8          |\n",
      "|    fps             | 101        |\n",
      "|    time_elapsed    | 279        |\n",
      "|    total_timesteps | 28192      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -14.3      |\n",
      "|    critic_loss     | 9.19       |\n",
      "|    learning_rate   | 0.001      |\n",
      "|    n_updates       | 28091      |\n",
      "|    reward          | -1.7623498 |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 12         |\n",
      "|    fps             | 100        |\n",
      "|    time_elapsed    | 419        |\n",
      "|    total_timesteps | 42288      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -14.6      |\n",
      "|    critic_loss     | 7.02       |\n",
      "|    learning_rate   | 0.001      |\n",
      "|    n_updates       | 42187      |\n",
      "|    reward          | -1.7623498 |\n",
      "-----------------------------------\n",
      "day: 3523, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7288347.61\n",
      "total_reward: 6288347.61\n",
      "total_cost: 1118.13\n",
      "total_trades: 66888\n",
      "Sharpe: 0.859\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=50000) if if_using_ddpg else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4eaf55b6-1cd1-4818-97fa-dc73174f7125",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d1da586f-098d-40f5-bdf2-364e00c1316e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to results/ppo\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "\n",
    "if if_using_ppo:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/ppo'\n",
    "  new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_ppo.set_logger(new_logger_ppo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "64513f5a-3a3b-476c-a274-306c7f48146e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 169        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 12         |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.61821795 |\n",
      "-----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014801119 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.0125     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.18        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    reward               | 0.16634023  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 18.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 171         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015971132 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.00775    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 54.1        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    reward               | -0.39887893 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 138         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 175          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 46           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0130170975 |\n",
      "|    clip_fraction        | 0.126        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -41.2        |\n",
      "|    explained_variance   | -0.0512      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 15           |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0202      |\n",
      "|    reward               | 0.19138955   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 33.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 178         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01697167  |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.00185    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 59.3        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    reward               | -0.37338564 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 126         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 180         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015443742 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.0226     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 34.8        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    reward               | 0.12938833  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 83.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 180         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 79          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016705468 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | 0.00782     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 39.7        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0178     |\n",
      "|    reward               | -1.2356745  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 65.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 181         |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 90          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018780988 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | -0.0138     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 107         |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    reward               | -13.88356   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 138         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 101         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022689702 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | -0.0288     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.2         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0216     |\n",
      "|    reward               | 0.16642037  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 18.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 112         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014922367 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | -0.00942    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 50.4        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    reward               | 3.6584747   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 127         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 182         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 123         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024055192 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.6       |\n",
      "|    explained_variance   | -0.0018     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 70.2        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0191     |\n",
      "|    reward               | 1.1577733   |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 69.3        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 183        |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 133        |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02354831 |\n",
      "|    clip_fraction        | 0.212      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.6      |\n",
      "|    explained_variance   | -0.00995   |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 42.5       |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0174    |\n",
      "|    reward               | -9.773501  |\n",
      "|    std                  | 1.02       |\n",
      "|    value_loss           | 74.8       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 184         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 144         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017276645 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.7       |\n",
      "|    explained_variance   | -0.00723    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 34.8        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00947    |\n",
      "|    reward               | 1.486841    |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 85          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 184         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 155         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027415331 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.7       |\n",
      "|    explained_variance   | 0.0192      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13          |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    reward               | 0.09063662  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 21          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 183         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 167         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024801359 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.8       |\n",
      "|    explained_variance   | -0.0296     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 71          |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00673    |\n",
      "|    reward               | 6.2609096   |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 131         |\n",
      "-----------------------------------------\n",
      "day: 3523, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 8343379.17\n",
      "total_reward: 7343379.17\n",
      "total_cost: 432338.78\n",
      "total_trades: 94569\n",
      "Sharpe: 0.917\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 184         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 177         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028364405 |\n",
      "|    clip_fraction        | 0.305       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.9       |\n",
      "|    explained_variance   | -0.0143     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.6        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    reward               | -0.09223497 |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 39.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 184         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 188         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019386802 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42         |\n",
      "|    explained_variance   | -0.00132    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 261         |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    reward               | -13.115124  |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 308         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 184          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 199          |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.027786132  |\n",
      "|    clip_fraction        | 0.269        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -42          |\n",
      "|    explained_variance   | -0.00041     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 77.1         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0056      |\n",
      "|    reward               | -0.107385226 |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 170          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 184         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 210         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025691086 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.1       |\n",
      "|    explained_variance   | 0.000677    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 92.2        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0116     |\n",
      "|    reward               | 0.34710947  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 185         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 184         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 221         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016662193 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.2       |\n",
      "|    explained_variance   | -0.00257    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 168         |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00595    |\n",
      "|    reward               | -0.5364817  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 302         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 184         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 232         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024410727 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.2       |\n",
      "|    explained_variance   | 0.000726    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.3        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    reward               | -4.4230957  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 27.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 185         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 243         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020891784 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.3       |\n",
      "|    explained_variance   | 0.0127      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 124         |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    reward               | 3.0180697   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 268         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 185         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 254         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.02595945  |\n",
      "|    clip_fraction        | 0.259       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.3       |\n",
      "|    explained_variance   | 0.0139      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 45.1        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00767    |\n",
      "|    reward               | -0.24443552 |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 87.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 185         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 264         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018293222 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.4       |\n",
      "|    explained_variance   | 0.0247      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 176         |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00996    |\n",
      "|    reward               | 5.471423    |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 384         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 185        |\n",
      "|    iterations           | 25         |\n",
      "|    time_elapsed         | 275        |\n",
      "|    total_timesteps      | 51200      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02805164 |\n",
      "|    clip_fraction        | 0.244      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.4      |\n",
      "|    explained_variance   | 0.00246    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 122        |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.00857   |\n",
      "|    reward               | 0.06380034 |\n",
      "|    std                  | 1.05       |\n",
      "|    value_loss           | 231        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 286         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020144444 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.5       |\n",
      "|    explained_variance   | -0.0122     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 22.4        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    reward               | -0.2001359  |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 87.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 296         |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021603968 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.00897     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 53.9        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00811    |\n",
      "|    reward               | 3.3100452   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 181         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 186        |\n",
      "|    iterations           | 28         |\n",
      "|    time_elapsed         | 307        |\n",
      "|    total_timesteps      | 57344      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02627321 |\n",
      "|    clip_fraction        | 0.256      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.6      |\n",
      "|    explained_variance   | 0.0789     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 13.2       |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    reward               | -2.579093  |\n",
      "|    std                  | 1.05       |\n",
      "|    value_loss           | 29.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 318         |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019028123 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.0278      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 119         |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0124     |\n",
      "|    reward               | -2.957849   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 398         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 328         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017106045 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.00885     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 63.2        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    reward               | -0.28774655 |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 128         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 339         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023826059 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.0118      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 42.4        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00914    |\n",
      "|    reward               | 0.38664338  |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 87.1        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 186        |\n",
      "|    iterations           | 32         |\n",
      "|    time_elapsed         | 350        |\n",
      "|    total_timesteps      | 65536      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02130727 |\n",
      "|    clip_fraction        | 0.182      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.8      |\n",
      "|    explained_variance   | 0.00842    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 318        |\n",
      "|    n_updates            | 310        |\n",
      "|    policy_gradient_loss | -0.0082    |\n",
      "|    reward               | -2.892793  |\n",
      "|    std                  | 1.06       |\n",
      "|    value_loss           | 545        |\n",
      "----------------------------------------\n",
      "day: 3523, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 13716562.27\n",
      "total_reward: 12716562.27\n",
      "total_cost: 323112.88\n",
      "total_trades: 87145\n",
      "Sharpe: 1.029\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 186         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 361         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016460631 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.0294      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13.4        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00714    |\n",
      "|    reward               | 0.97709364  |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 28.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 372         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018639915 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.9       |\n",
      "|    explained_variance   | 0.0134      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 406         |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00789    |\n",
      "|    reward               | 2.558707    |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 660         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 383         |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018164616 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.9       |\n",
      "|    explained_variance   | 0.0328      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 37.6        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00191    |\n",
      "|    reward               | 0.2946861   |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 102         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 187        |\n",
      "|    iterations           | 36         |\n",
      "|    time_elapsed         | 393        |\n",
      "|    total_timesteps      | 73728      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01404178 |\n",
      "|    clip_fraction        | 0.142      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43        |\n",
      "|    explained_variance   | 0.0442     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 319        |\n",
      "|    n_updates            | 350        |\n",
      "|    policy_gradient_loss | -0.00871   |\n",
      "|    reward               | -19.292614 |\n",
      "|    std                  | 1.07       |\n",
      "|    value_loss           | 635        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 404         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011708469 |\n",
      "|    clip_fraction        | 0.0833      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43         |\n",
      "|    explained_variance   | -0.00286    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 124         |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00358    |\n",
      "|    reward               | 3.0823612   |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 272         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 187          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 415          |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.02290388   |\n",
      "|    clip_fraction        | 0.223        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -43.1        |\n",
      "|    explained_variance   | 0.0577       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 37.6         |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.015       |\n",
      "|    reward               | -0.044113964 |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 140          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 426         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017325101 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.1       |\n",
      "|    explained_variance   | 0.0435      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 379         |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00438    |\n",
      "|    reward               | 5.8977413   |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 646         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 436         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01239772  |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.1       |\n",
      "|    explained_variance   | 0.409       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 15.8        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00857    |\n",
      "|    reward               | -0.09081244 |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 32.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 447         |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015639493 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.1       |\n",
      "|    explained_variance   | 0.0976      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 374         |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00451    |\n",
      "|    reward               | 0.41384327  |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 532         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 457         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010820602 |\n",
      "|    clip_fraction        | 0.0726      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.2       |\n",
      "|    explained_variance   | 0.0466      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 141         |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.00947    |\n",
      "|    reward               | -2.562341   |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 230         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 468         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014384112 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.2       |\n",
      "|    explained_variance   | 0.159       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 269         |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    reward               | -34.10567   |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 321         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 188        |\n",
      "|    iterations           | 44         |\n",
      "|    time_elapsed         | 479        |\n",
      "|    total_timesteps      | 90112      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01892119 |\n",
      "|    clip_fraction        | 0.223      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.2      |\n",
      "|    explained_variance   | 0.00934    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 476        |\n",
      "|    n_updates            | 430        |\n",
      "|    policy_gradient_loss | -0.00707   |\n",
      "|    reward               | -0.9531242 |\n",
      "|    std                  | 1.07       |\n",
      "|    value_loss           | 834        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 489         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013677264 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.2       |\n",
      "|    explained_variance   | 0.459       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 33.1        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    reward               | 0.33405423  |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 56          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 500         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020553224 |\n",
      "|    clip_fraction        | 0.211       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.2       |\n",
      "|    explained_variance   | 0.107       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 414         |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00347    |\n",
      "|    reward               | -5.6133347  |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 1.03e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 512         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027015429 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.3       |\n",
      "|    explained_variance   | 0.19        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 47.2        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | 0.00706     |\n",
      "|    reward               | 2.0708926   |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 127         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 523         |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016423114 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.3       |\n",
      "|    explained_variance   | 0.15        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 378         |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.00675    |\n",
      "|    reward               | -28.99694   |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 972         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 534         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017076619 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.4       |\n",
      "|    explained_variance   | 0.0295      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 164         |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.00805    |\n",
      "|    reward               | -4.706502   |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 509         |\n",
      "-----------------------------------------\n",
      "day: 3523, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 13798326.48\n",
      "total_reward: 12798326.48\n",
      "total_cost: 346614.66\n",
      "total_trades: 87814\n",
      "Sharpe: 1.070\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 545         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016095275 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.4       |\n",
      "|    explained_variance   | 0.147       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 169         |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.00796    |\n",
      "|    reward               | -1.5465951  |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 458         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 556         |\n",
      "|    total_timesteps      | 104448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027883198 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.4       |\n",
      "|    explained_variance   | 0.08        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 494         |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.00119    |\n",
      "|    reward               | -0.27344358 |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 750         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 187        |\n",
      "|    iterations           | 52         |\n",
      "|    time_elapsed         | 567        |\n",
      "|    total_timesteps      | 106496     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02609051 |\n",
      "|    clip_fraction        | 0.211      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.4      |\n",
      "|    explained_variance   | 0.776      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 12.6       |\n",
      "|    n_updates            | 510        |\n",
      "|    policy_gradient_loss | -0.00773   |\n",
      "|    reward               | 0.89336115 |\n",
      "|    std                  | 1.08       |\n",
      "|    value_loss           | 27         |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 577         |\n",
      "|    total_timesteps      | 108544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005946842 |\n",
      "|    clip_fraction        | 0.0325      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.5       |\n",
      "|    explained_variance   | 0.259       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 184         |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.00327    |\n",
      "|    reward               | 57.398552   |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 469         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 588         |\n",
      "|    total_timesteps      | 110592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012642576 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.5       |\n",
      "|    explained_variance   | 0.266       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 89.4        |\n",
      "|    n_updates            | 530         |\n",
      "|    policy_gradient_loss | -0.00983    |\n",
      "|    reward               | 1.5035214   |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 196         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 599         |\n",
      "|    total_timesteps      | 112640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035739094 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.5       |\n",
      "|    explained_variance   | 0.337       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 208         |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | 0.00209     |\n",
      "|    reward               | -10.248956  |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 367         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 610         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017438453 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.5       |\n",
      "|    explained_variance   | 0.0199      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 175         |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.00442    |\n",
      "|    reward               | 0.8601208   |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 627         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 621         |\n",
      "|    total_timesteps      | 116736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025250822 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.6       |\n",
      "|    explained_variance   | 0.32        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 80.6        |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0103     |\n",
      "|    reward               | -0.09761629 |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 122         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 631         |\n",
      "|    total_timesteps      | 118784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019788163 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.6       |\n",
      "|    explained_variance   | 0.169       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 422         |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.00603    |\n",
      "|    reward               | 14.525853   |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 776         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 642         |\n",
      "|    total_timesteps      | 120832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034841195 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.6       |\n",
      "|    explained_variance   | 0.602       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 20.1        |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.00648    |\n",
      "|    reward               | -0.06474486 |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 50.5        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 188        |\n",
      "|    iterations           | 60         |\n",
      "|    time_elapsed         | 653        |\n",
      "|    total_timesteps      | 122880     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01932012 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.7      |\n",
      "|    explained_variance   | 0.236      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 416        |\n",
      "|    n_updates            | 590        |\n",
      "|    policy_gradient_loss | -0.00712   |\n",
      "|    reward               | -7.5322046 |\n",
      "|    std                  | 1.09       |\n",
      "|    value_loss           | 689        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 188        |\n",
      "|    iterations           | 61         |\n",
      "|    time_elapsed         | 663        |\n",
      "|    total_timesteps      | 124928     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03826729 |\n",
      "|    clip_fraction        | 0.178      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.7      |\n",
      "|    explained_variance   | 0.185      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 71.8       |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | 0.00111    |\n",
      "|    reward               | -2.5724578 |\n",
      "|    std                  | 1.09       |\n",
      "|    value_loss           | 273        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 674         |\n",
      "|    total_timesteps      | 126976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015218491 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.7       |\n",
      "|    explained_variance   | 0.147       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 227         |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.00669    |\n",
      "|    reward               | -1.2576073  |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 436         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 685         |\n",
      "|    total_timesteps      | 129024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031884633 |\n",
      "|    clip_fraction        | 0.348       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.7       |\n",
      "|    explained_variance   | 0.0649      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 189         |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.00574    |\n",
      "|    reward               | -0.10471362 |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 419         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 188        |\n",
      "|    iterations           | 64         |\n",
      "|    time_elapsed         | 696        |\n",
      "|    total_timesteps      | 131072     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02489777 |\n",
      "|    clip_fraction        | 0.149      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.7      |\n",
      "|    explained_variance   | 0.728      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 10.7       |\n",
      "|    n_updates            | 630        |\n",
      "|    policy_gradient_loss | -0.00887   |\n",
      "|    reward               | 0.23367724 |\n",
      "|    std                  | 1.09       |\n",
      "|    value_loss           | 21         |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 706         |\n",
      "|    total_timesteps      | 133120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017053245 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.8       |\n",
      "|    explained_variance   | 0.138       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 223         |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.00333    |\n",
      "|    reward               | 3.1568134   |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 420         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 717         |\n",
      "|    total_timesteps      | 135168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022323107 |\n",
      "|    clip_fraction        | 0.309       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.8       |\n",
      "|    explained_variance   | 0.186       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 65.6        |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.00143    |\n",
      "|    reward               | 0.07177788  |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 88.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 728         |\n",
      "|    total_timesteps      | 137216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022380311 |\n",
      "|    clip_fraction        | 0.265       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.8       |\n",
      "|    explained_variance   | 0.201       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 306         |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.00547    |\n",
      "|    reward               | 2.1368098   |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 523         |\n",
      "-----------------------------------------\n",
      "day: 3523, episode: 70\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 13877700.12\n",
      "total_reward: 12877700.12\n",
      "total_cost: 230806.99\n",
      "total_trades: 80494\n",
      "Sharpe: 1.049\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 739         |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023448456 |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.9       |\n",
      "|    explained_variance   | 0.0543      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 157         |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.00147    |\n",
      "|    reward               | -2.014443   |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 478         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 750         |\n",
      "|    total_timesteps      | 141312      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030311573 |\n",
      "|    clip_fraction        | 0.25        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.9       |\n",
      "|    explained_variance   | 0.288       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 71.1        |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.00342    |\n",
      "|    reward               | 3.1114554   |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 238         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 761         |\n",
      "|    total_timesteps      | 143360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023997715 |\n",
      "|    clip_fraction        | 0.266       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44         |\n",
      "|    explained_variance   | 0.151       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 406         |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.000816   |\n",
      "|    reward               | -9.630689   |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 838         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 772         |\n",
      "|    total_timesteps      | 145408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038432933 |\n",
      "|    clip_fraction        | 0.271       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 0.606       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 18.9        |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | 0.00256     |\n",
      "|    reward               | 1.0787202   |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 46.5        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 188        |\n",
      "|    iterations           | 72         |\n",
      "|    time_elapsed         | 783        |\n",
      "|    total_timesteps      | 147456     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01907675 |\n",
      "|    clip_fraction        | 0.214      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.2      |\n",
      "|    explained_variance   | 0.259      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 312        |\n",
      "|    n_updates            | 710        |\n",
      "|    policy_gradient_loss | -0.0066    |\n",
      "|    reward               | 5.2311206  |\n",
      "|    std                  | 1.11       |\n",
      "|    value_loss           | 706        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 188        |\n",
      "|    iterations           | 73         |\n",
      "|    time_elapsed         | 793        |\n",
      "|    total_timesteps      | 149504     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01969093 |\n",
      "|    clip_fraction        | 0.183      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.3      |\n",
      "|    explained_variance   | 0.167      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 53.6       |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | 0.00169    |\n",
      "|    reward               | 1.3023193  |\n",
      "|    std                  | 1.11       |\n",
      "|    value_loss           | 214        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 188        |\n",
      "|    iterations           | 74         |\n",
      "|    time_elapsed         | 804        |\n",
      "|    total_timesteps      | 151552     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01719685 |\n",
      "|    clip_fraction        | 0.183      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.3      |\n",
      "|    explained_variance   | 0.324      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 119        |\n",
      "|    n_updates            | 730        |\n",
      "|    policy_gradient_loss | -0.00896   |\n",
      "|    reward               | 0.10388767 |\n",
      "|    std                  | 1.12       |\n",
      "|    value_loss           | 260        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 815         |\n",
      "|    total_timesteps      | 153600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015176894 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.3       |\n",
      "|    explained_variance   | 0.0549      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 178         |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.00464    |\n",
      "|    reward               | -0.31698775 |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 298         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 188        |\n",
      "|    iterations           | 76         |\n",
      "|    time_elapsed         | 826        |\n",
      "|    total_timesteps      | 155648     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02989291 |\n",
      "|    clip_fraction        | 0.313      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.4      |\n",
      "|    explained_variance   | 0.753      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 8.62       |\n",
      "|    n_updates            | 750        |\n",
      "|    policy_gradient_loss | -0.00323   |\n",
      "|    reward               | 0.63170874 |\n",
      "|    std                  | 1.12       |\n",
      "|    value_loss           | 24.9       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 837         |\n",
      "|    total_timesteps      | 157696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020527098 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.4       |\n",
      "|    explained_variance   | 0.267       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 168         |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.0037     |\n",
      "|    reward               | -0.19614944 |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 383         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 848         |\n",
      "|    total_timesteps      | 159744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019351574 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.4       |\n",
      "|    explained_variance   | 0.322       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 31.6        |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.00516    |\n",
      "|    reward               | -0.35282236 |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 60.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 858         |\n",
      "|    total_timesteps      | 161792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019645397 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.5       |\n",
      "|    explained_variance   | 0.33        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 169         |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.00789    |\n",
      "|    reward               | 3.508591    |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 374         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 870         |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013780334 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.5       |\n",
      "|    explained_variance   | 0.0383      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 220         |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.00329    |\n",
      "|    reward               | 1.7815127   |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 325         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 881         |\n",
      "|    total_timesteps      | 165888      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019342504 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.5       |\n",
      "|    explained_variance   | 0.351       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 170         |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -0.00107    |\n",
      "|    reward               | 0.8400871   |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 247         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 894         |\n",
      "|    total_timesteps      | 167936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021691611 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.6       |\n",
      "|    explained_variance   | 0.218       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 155         |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.00251    |\n",
      "|    reward               | -3.3415496  |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 360         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 906         |\n",
      "|    total_timesteps      | 169984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040912163 |\n",
      "|    clip_fraction        | 0.322       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.6       |\n",
      "|    explained_variance   | 0.494       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 20.8        |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.00042    |\n",
      "|    reward               | -1.1685984  |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 32.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 917         |\n",
      "|    total_timesteps      | 172032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039406706 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.6       |\n",
      "|    explained_variance   | 0.173       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 164         |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.00477    |\n",
      "|    reward               | 3.4746764   |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 222         |\n",
      "-----------------------------------------\n",
      "day: 3523, episode: 80\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 9918382.15\n",
      "total_reward: 8918382.15\n",
      "total_cost: 279326.52\n",
      "total_trades: 85098\n",
      "Sharpe: 0.927\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 927         |\n",
      "|    total_timesteps      | 174080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01776499  |\n",
      "|    clip_fraction        | 0.223       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.7       |\n",
      "|    explained_variance   | 0.164       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 52.4        |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.00265    |\n",
      "|    reward               | -0.29164082 |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 137         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 938         |\n",
      "|    total_timesteps      | 176128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016513376 |\n",
      "|    clip_fraction        | 0.13        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.7       |\n",
      "|    explained_variance   | 0.26        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 89.1        |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.00738    |\n",
      "|    reward               | -1.7534345  |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 172         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 187        |\n",
      "|    iterations           | 87         |\n",
      "|    time_elapsed         | 948        |\n",
      "|    total_timesteps      | 178176     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00394288 |\n",
      "|    clip_fraction        | 0.0208     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.7      |\n",
      "|    explained_variance   | 0.0789     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 306        |\n",
      "|    n_updates            | 860        |\n",
      "|    policy_gradient_loss | -0.00283   |\n",
      "|    reward               | 10.125111  |\n",
      "|    std                  | 1.13       |\n",
      "|    value_loss           | 570        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 959         |\n",
      "|    total_timesteps      | 180224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012952814 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.7       |\n",
      "|    explained_variance   | 0.549       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 28.7        |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    reward               | 0.6468935   |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 62.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 970         |\n",
      "|    total_timesteps      | 182272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011604515 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.7       |\n",
      "|    explained_variance   | 0.215       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 225         |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.00424    |\n",
      "|    reward               | -1.186503   |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 662         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 981         |\n",
      "|    total_timesteps      | 184320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016902346 |\n",
      "|    clip_fraction        | 0.188       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.7       |\n",
      "|    explained_variance   | 0.566       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 66.4        |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.00417    |\n",
      "|    reward               | 0.7415024   |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 97.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 991         |\n",
      "|    total_timesteps      | 186368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019953974 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.7       |\n",
      "|    explained_variance   | 0.276       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 383         |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | 0.00155     |\n",
      "|    reward               | 0.3055018   |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 714         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 1002        |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012428301 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.7       |\n",
      "|    explained_variance   | 0.244       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 167         |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.00374    |\n",
      "|    reward               | -1.7555714  |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 397         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 1013        |\n",
      "|    total_timesteps      | 190464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.01192046  |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.7       |\n",
      "|    explained_variance   | 0.403       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 120         |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.00753    |\n",
      "|    reward               | -0.39710483 |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 335         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 187          |\n",
      "|    iterations           | 94           |\n",
      "|    time_elapsed         | 1024         |\n",
      "|    total_timesteps      | 192512       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071536773 |\n",
      "|    clip_fraction        | 0.0735       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -44.8        |\n",
      "|    explained_variance   | 0.183        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 398          |\n",
      "|    n_updates            | 930          |\n",
      "|    policy_gradient_loss | -0.00219     |\n",
      "|    reward               | 10.579326    |\n",
      "|    std                  | 1.13         |\n",
      "|    value_loss           | 741          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 1035        |\n",
      "|    total_timesteps      | 194560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023406856 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.8       |\n",
      "|    explained_variance   | 0.788       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.9        |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.00172    |\n",
      "|    reward               | -3.1399364  |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 31.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 187         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 1046        |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008692204 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.8       |\n",
      "|    explained_variance   | 0.288       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 322         |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.00439    |\n",
      "|    reward               | -26.076866  |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 663         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 187        |\n",
      "|    iterations           | 97         |\n",
      "|    time_elapsed         | 1057       |\n",
      "|    total_timesteps      | 198656     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01416049 |\n",
      "|    clip_fraction        | 0.14       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.8      |\n",
      "|    explained_variance   | 0.265      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 124        |\n",
      "|    n_updates            | 960        |\n",
      "|    policy_gradient_loss | -0.00107   |\n",
      "|    reward               | -0.5509661 |\n",
      "|    std                  | 1.14       |\n",
      "|    value_loss           | 272        |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 187          |\n",
      "|    iterations           | 98           |\n",
      "|    time_elapsed         | 1067         |\n",
      "|    total_timesteps      | 200704       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0153616695 |\n",
      "|    clip_fraction        | 0.153        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -44.9        |\n",
      "|    explained_variance   | 0.312        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 365          |\n",
      "|    n_updates            | 970          |\n",
      "|    policy_gradient_loss | -0.0026      |\n",
      "|    reward               | -41.768124   |\n",
      "|    std                  | 1.14         |\n",
      "|    value_loss           | 804          |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_ppo = agent.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=200000) if if_using_ppo else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f725b945-5554-4090-8d2f-a806e0aac942",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9e48470c-a807-4fd9-940c-dd21c2b65c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "Logging to results/td3\n"
     ]
    }
   ],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "TD3_PARAMS = {\"batch_size\": 100, \n",
    "              \"buffer_size\": 1000000, \n",
    "              \"learning_rate\": 0.001}\n",
    "\n",
    "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
    "\n",
    "if if_using_td3:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/td3'\n",
    "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_td3.set_logger(new_logger_td3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22896227-a471-464a-8f6e-34811c07e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_td3 = agent.train_model(model=model_td3, \n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=50000) if if_using_td3 else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20e38d6-8219-47c8-9b55-b5ef76f5cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bfad9f-2877-47ec-ad62-e84c71e9fef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "SAC_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "if if_using_sac:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/sac'\n",
    "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_sac.set_logger(new_logger_sac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f42576-fe33-45a7-8a37-c610c9ac9e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_sac = agent.train_model(model=model_sac, \n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=70000) if if_using_sac else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573b7286-94db-404d-ac31-9b63ad07a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9a17b6-990c-480f-81bb-58d2e309d685",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "trade = pd.read_csv('trade_data.csv')\n",
    "\n",
    "# If you are not using the data generated from part 1 of this tutorial, make sure \n",
    "# it has the columns and index in the form that could be make into the environment. \n",
    "# Then you can comment and skip the following lines.\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = ['']\n",
    "trade = trade.set_index(trade.columns[0])\n",
    "trade.index.names = ['']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8e3cd5-241c-409d-9ea8-76eb05548988",
   "metadata": {},
   "outputs": [],
   "source": [
    "if_using_a2c = True\n",
    "if_using_ddpg = True\n",
    "if_using_ppo = True\n",
    "if_using_td3 = True\n",
    "if_using_sac = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf77799f-e77a-4b28-b01f-427fbff55023",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_a2c = A2C.load(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None\n",
    "trained_ddpg = DDPG.load(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None\n",
    "trained_ppo = PPO.load(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None\n",
    "trained_td3 = TD3.load(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None\n",
    "trained_sac = SAC.load(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517320ac-5962-421b-b497-e74069d5d33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_dimension = len(trade.tic.unique())\n",
    "state_space = 1 + 2 * stock_dimension + len(INDICATORS) * stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a79f44-66df-4115-b45d-32a405e925c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ed7b98-97da-4150-854c-fa646470cf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_trade_gym = StockTradingEnv(df = trade, turbulence_threshold = 70,risk_indicator_col='vix', **env_kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c2ce9b-51f9-47ea-896e-85b761058732",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_account_value_a2c, df_actions_a2c = DRLAgent.DRL_prediction(\n",
    "    model=trained_a2c, \n",
    "    environment = e_trade_gym) if if_using_a2c else (None, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5d6059-2875-47dd-9929-3fe5e6f0035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_account_value_ddpg, df_actions_ddpg = DRLAgent.DRL_prediction(\n",
    "    model=trained_ddpg, \n",
    "    environment = e_trade_gym) if if_using_ddpg else (None, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92220d39-7ca6-44cf-b5ae-d73c29282ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_account_value_ppo, df_actions_ppo = DRLAgent.DRL_prediction(\n",
    "    model=trained_ppo, \n",
    "    environment = e_trade_gym) if if_using_ppo else (None, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841b4310-65a1-4d3b-8530-ce468add6a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_account_value_td3, df_actions_td3 = DRLAgent.DRL_prediction(\n",
    "    model=trained_td3, \n",
    "    environment = e_trade_gym) if if_using_td3 else (None, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cea5585-9dfd-4fc2-9738-d8053b1be858",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_account_value_sac, df_actions_sac = DRLAgent.DRL_prediction(\n",
    "    model=trained_sac, \n",
    "    environment = e_trade_gym) if if_using_sac else (None, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe608f7-16bc-415b-8825-aed9e51d6f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_a2c = (\n",
    "    df_account_value_a2c.set_index(df_account_value_a2c.columns[0])\n",
    "    if if_using_a2c\n",
    "    else None\n",
    ")\n",
    "df_result_ddpg = (\n",
    "    df_account_value_ddpg.set_index(df_account_value_ddpg.columns[0])\n",
    "    if if_using_ddpg\n",
    "    else None\n",
    ")\n",
    "df_result_ppo = (\n",
    "    df_account_value_ppo.set_index(df_account_value_ppo.columns[0])\n",
    "    if if_using_ppo\n",
    "    else None\n",
    ")\n",
    "df_result_td3 = (\n",
    "    df_account_value_td3.set_index(df_account_value_td3.columns[0])\n",
    "    if if_using_td3\n",
    "    else None\n",
    ")\n",
    "df_result_sac = (\n",
    "    df_account_value_sac.set_index(df_account_value_sac.columns[0])\n",
    "    if if_using_sac\n",
    "    else None\n",
    ")\n",
    "\n",
    "result = pd.DataFrame(\n",
    "    {\n",
    "        \"a2c\": df_result_a2c[\"account_value\"] if if_using_a2c else None,\n",
    "        \"ddpg\": df_result_ddpg[\"account_value\"] if if_using_ddpg else None,\n",
    "        \"ppo\": df_result_ppo[\"account_value\"] if if_using_ppo else None,\n",
    "        \"td3\": df_result_td3[\"account_value\"] if if_using_td3 else None,\n",
    "        \"sac\": df_result_sac[\"account_value\"] if if_using_sac else None,\n",
    "        \"mvo\": MVO_result[\"Mean Var\"],\n",
    "        \"dji\": dji[\"close\"],\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff2cb68-36ca-4f80-93df-702b75478a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
